{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0a4301a-1e1f-4dbf-a04f-7aeedc338555",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 03:06:40.851348: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-26 03:06:46.595603: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-26 03:06:58.172718: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d555589b-edf4-4169-8382-d7ad7f98c0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c57ef0ae9bd84d12a162f3c09c852e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'peft_model_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m LlamaForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/shared/4/models/llama2/pytorch-versions/llama-2-7b/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load the Lora model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m PeftModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model, \u001b[43mpeft_model_id\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'peft_model_id' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(\"/shared/4/models/llama2/\")\n",
    "model = LlamaForCausalLM.from_pretrained(\"/shared/4/models/llama2/pytorch-versions/llama-2-7b/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea613152-8a62-47e4-b0b5-75f87dba9ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s>Dog is a domesticated descendant of the wolf. Dog is a member of the genus Canis, which forms part of the wolf-like canids, and was the first species and the only large carnivore to have'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs = tokenizer('Dog is', return_tensors='pt')\n",
    "greedy_output = model.generate(**model_inputs, max_length=50)\n",
    "tokenizer.decode(greedy_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab8f54af-9fe8-48fa-a163-9432b2382c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Lora model\n",
    "peft_model_id = \"/shared/2/projects/llm-personas/llama2-4chan-lora/checkpoint-final\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77a2a0ee-f245-4276-9cef-0efd5cffbd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<s>Dog is a man's best friend. Dog. Man. Friend. Best.  Dog'll be your best buddy. He'd do anything for you. You can trust him. And he'l never let you down.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs = tokenizer('Dog is', return_tensors='pt')\n",
    "greedy_output = model.generate(**model_inputs, no_repeat_ngram_size=2, max_length=50)\n",
    "tokenizer.decode(greedy_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6974cd0-6d24-48c8-b17e-882ded3a7940",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<s>I am happy today, I have a lot of things to do, and I will do them. I don't know what I'm doing, but I know I want to be happy. So I do things that make me happy,\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs = tokenizer('I am happy today,', return_tensors='pt')\n",
    "greedy_output = model.generate(**model_inputs, no_repeat_ngram_size=2, max_length=50)\n",
    "tokenizer.decode(greedy_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a651041-37d1-4aac-b409-8e23bfa4fd66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/home/leczhang/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 57, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s>### Instruction: Answer whether you agree with the statement or not. Reply with either No or Yes.\\n ### Input: Statement: The existence of potential harm to others is not always wrong, irrespective of the benefits to be gained.\\n ### Your Response: Yes'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inst = \"### Instruction: Answer whether you agree with the statement or not. Reply with either No or Yes.\\n ### Input: Statement: The existence of potential harm to others is not always wrong, irrespective of the benefits to be gained.\\n ### Your Response:\"\n",
    "model_inputs = tokenizer(inst, return_tensors='pt')\n",
    "greedy_output = model.generate(**model_inputs, no_repeat_ngram_size=2, max_length=50)\n",
    "tokenizer.decode(greedy_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaf2bdb-02ec-41e1-a57e-71a67074eb4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81aff6da-ad7d-47a8-8374-5c114440536b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "846b9a6dad404bb6ae82f3a895278f7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(\"/shared/4/models/llama2/\")\n",
    "model = LlamaForCausalLM.from_pretrained(\"/shared/2/projects/llm-personas/llama2-4chan/checkpoint-final/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "401111ff-183f-45e2-a8e7-b04e77420f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<s>Dog is a male name. What is the female version of dog? Cat. Why do they call it a cunt if it's a dude? I don't know but I'm going to assume it has something to do\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs = tokenizer('Dog is', return_tensors='pt')\n",
    "greedy_output = model.generate(**model_inputs, no_repeat_ngram_size=2, max_length=50)\n",
    "tokenizer.decode(greedy_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da1376c4-6e42-4f02-b03d-c3802266605d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/home/leczhang/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 93, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<s>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. ### Instruction: You will be presented with a statement. Answer whether you agree with it or not. Reply with only 'Yes' or 'No' without explaining your reasoning. ### Input: Statement: Moral actions are those which closely match ideals of the most perfect action. ### Response: Yes\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inst = \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. ### Instruction: You will be presented with a statement. Answer whether you agree with it or not. Reply with only 'Yes' or 'No' without explaining your reasoning. ### Input: Statement: Moral actions are those which closely match ideals of the most perfect action. ### Response:\"\n",
    "model_inputs = tokenizer(inst, return_tensors='pt')\n",
    "greedy_output = model.generate(**model_inputs, no_repeat_ngram_size=2, max_length=50)\n",
    "tokenizer.decode(greedy_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34105a9-2850-4840-a83d-1feb0bfc07ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
