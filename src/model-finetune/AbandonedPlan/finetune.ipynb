{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecb9d1fc-ebcb-4bf4-8aa0-3d4e1fa819f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "import textwrap\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    ")\n",
    "\n",
    "import fire\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(rc={'figure.figsize':(10, 7)})\n",
    "sns.set(rc={'figure.dpi':100})\n",
    "sns.set(style='white', palette='muted', font_scale=1.2)\n",
    "\n",
    "'''\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8abd106-a853-4aab-9d14-a0e19b52c4fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "/shared/4/models/llama2 does not appear to have a file named config.json. Checkout 'https://huggingface.co//shared/4/models/llama2/None' for available files.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM\n\u001b[0;32m----> 2\u001b[0m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/shared/4/models/llama2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:527\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    525\u001b[0m     _ \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 527\u001b[0m config, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs_orig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1023\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1019\u001b[0m revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrevision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1021\u001b[0m revision \u001b[38;5;241m=\u001b[39m sanitize_code_revision(pretrained_model_name_or_path, revision, trust_remote_code)\n\u001b[0;32m-> 1023\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1027\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/configuration_utils.py:620\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m    622\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/configuration_utils.py:675\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    671\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME)\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    689\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/hub.py:400\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(resolved_file):\n\u001b[1;32m    399\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _raise_exceptions_for_missing_entries:\n\u001b[0;32m--> 400\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    401\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Checkout \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    402\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    403\u001b[0m         )\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: /shared/4/models/llama2 does not appear to have a file named config.json. Checkout 'https://huggingface.co//shared/4/models/llama2/None' for available files."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "AutoModelForCausalLM.from_pretrained('/shared/4/models/llama2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "69e5bcc0-f3a7-4808-a96a-8effab0b0423",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaConfig\n",
    "#config = LlamaConfig.from_pretrained('/shared/4/models/llama2/llama-2-7b/params.json')\n",
    "config = LlamaConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6d5c4fee-b06c-45b0-b895-f88bf54c8912",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for LlamaForCausalLM:\n\tMissing key(s) in state_dict: \"model.embed_tokens.weight\", \"model.layers.0.self_attn.q_proj.weight\", \"model.layers.0.self_attn.k_proj.weight\", \"model.layers.0.self_attn.v_proj.weight\", \"model.layers.0.self_attn.o_proj.weight\", \"model.layers.0.mlp.gate_proj.weight\", \"model.layers.0.mlp.up_proj.weight\", \"model.layers.0.mlp.down_proj.weight\", \"model.layers.0.input_layernorm.weight\", \"model.layers.0.post_attention_layernorm.weight\", \"model.layers.1.self_attn.q_proj.weight\", \"model.layers.1.self_attn.k_proj.weight\", \"model.layers.1.self_attn.v_proj.weight\", \"model.layers.1.self_attn.o_proj.weight\", \"model.layers.1.mlp.gate_proj.weight\", \"model.layers.1.mlp.up_proj.weight\", \"model.layers.1.mlp.down_proj.weight\", \"model.layers.1.input_layernorm.weight\", \"model.layers.1.post_attention_layernorm.weight\", \"model.layers.2.self_attn.q_proj.weight\", \"model.layers.2.self_attn.k_proj.weight\", \"model.layers.2.self_attn.v_proj.weight\", \"model.layers.2.self_attn.o_proj.weight\", \"model.layers.2.mlp.gate_proj.weight\", \"model.layers.2.mlp.up_proj.weight\", \"model.layers.2.mlp.down_proj.weight\", \"model.layers.2.input_layernorm.weight\", \"model.layers.2.post_attention_layernorm.weight\", \"model.layers.3.self_attn.q_proj.weight\", \"model.layers.3.self_attn.k_proj.weight\", \"model.layers.3.self_attn.v_proj.weight\", \"model.layers.3.self_attn.o_proj.weight\", \"model.layers.3.mlp.gate_proj.weight\", \"model.layers.3.mlp.up_proj.weight\", \"model.layers.3.mlp.down_proj.weight\", \"model.layers.3.input_layernorm.weight\", \"model.layers.3.post_attention_layernorm.weight\", \"model.layers.4.self_attn.q_proj.weight\", \"model.layers.4.self_attn.k_proj.weight\", \"model.layers.4.self_attn.v_proj.weight\", \"model.layers.4.self_attn.o_proj.weight\", \"model.layers.4.mlp.gate_proj.weight\", \"model.layers.4.mlp.up_proj.weight\", \"model.layers.4.mlp.down_proj.weight\", \"model.layers.4.input_layernorm.weight\", \"model.layers.4.post_attention_layernorm.weight\", \"model.layers.5.self_attn.q_proj.weight\", \"model.layers.5.self_attn.k_proj.weight\", \"model.layers.5.self_attn.v_proj.weight\", \"model.layers.5.self_attn.o_proj.weight\", \"model.layers.5.mlp.gate_proj.weight\", \"model.layers.5.mlp.up_proj.weight\", \"model.layers.5.mlp.down_proj.weight\", \"model.layers.5.input_layernorm.weight\", \"model.layers.5.post_attention_layernorm.weight\", \"model.layers.6.self_attn.q_proj.weight\", \"model.layers.6.self_attn.k_proj.weight\", \"model.layers.6.self_attn.v_proj.weight\", \"model.layers.6.self_attn.o_proj.weight\", \"model.layers.6.mlp.gate_proj.weight\", \"model.layers.6.mlp.up_proj.weight\", \"model.layers.6.mlp.down_proj.weight\", \"model.layers.6.input_layernorm.weight\", \"model.layers.6.post_attention_layernorm.weight\", \"model.layers.7.self_attn.q_proj.weight\", \"model.layers.7.self_attn.k_proj.weight\", \"model.layers.7.self_attn.v_proj.weight\", \"model.layers.7.self_attn.o_proj.weight\", \"model.layers.7.mlp.gate_proj.weight\", \"model.layers.7.mlp.up_proj.weight\", \"model.layers.7.mlp.down_proj.weight\", \"model.layers.7.input_layernorm.weight\", \"model.layers.7.post_attention_layernorm.weight\", \"model.layers.8.self_attn.q_proj.weight\", \"model.layers.8.self_attn.k_proj.weight\", \"model.layers.8.self_attn.v_proj.weight\", \"model.layers.8.self_attn.o_proj.weight\", \"model.layers.8.mlp.gate_proj.weight\", \"model.layers.8.mlp.up_proj.weight\", \"model.layers.8.mlp.down_proj.weight\", \"model.layers.8.input_layernorm.weight\", \"model.layers.8.post_attention_layernorm.weight\", \"model.layers.9.self_attn.q_proj.weight\", \"model.layers.9.self_attn.k_proj.weight\", \"model.layers.9.self_attn.v_proj.weight\", \"model.layers.9.self_attn.o_proj.weight\", \"model.layers.9.mlp.gate_proj.weight\", \"model.layers.9.mlp.up_proj.weight\", \"model.layers.9.mlp.down_proj.weight\", \"model.layers.9.input_layernorm.weight\", \"model.layers.9.post_attention_layernorm.weight\", \"model.layers.10.self_attn.q_proj.weight\", \"model.layers.10.self_attn.k_proj.weight\", \"model.layers.10.self_attn.v_proj.weight\", \"model.layers.10.self_attn.o_proj.weight\", \"model.layers.10.mlp.gate_proj.weight\", \"model.layers.10.mlp.up_proj.weight\", \"model.layers.10.mlp.down_proj.weight\", \"model.layers.10.input_layernorm.weight\", \"model.layers.10.post_attention_layernorm.weight\", \"model.layers.11.self_attn.q_proj.weight\", \"model.layers.11.self_attn.k_proj.weight\", \"model.layers.11.self_attn.v_proj.weight\", \"model.layers.11.self_attn.o_proj.weight\", \"model.layers.11.mlp.gate_proj.weight\", \"model.layers.11.mlp.up_proj.weight\", \"model.layers.11.mlp.down_proj.weight\", \"model.layers.11.input_layernorm.weight\", \"model.layers.11.post_attention_layernorm.weight\", \"model.layers.12.self_attn.q_proj.weight\", \"model.layers.12.self_attn.k_proj.weight\", \"model.layers.12.self_attn.v_proj.weight\", \"model.layers.12.self_attn.o_proj.weight\", \"model.layers.12.mlp.gate_proj.weight\", \"model.layers.12.mlp.up_proj.weight\", \"model.layers.12.mlp.down_proj.weight\", \"model.layers.12.input_layernorm.weight\", \"model.layers.12.post_attention_layernorm.weight\", \"model.layers.13.self_attn.q_proj.weight\", \"model.layers.13.self_attn.k_proj.weight\", \"model.layers.13.self_attn.v_proj.weight\", \"model.layers.13.self_attn.o_proj.weight\", \"model.layers.13.mlp.gate_proj.weight\", \"model.layers.13.mlp.up_proj.weight\", \"model.layers.13.mlp.down_proj.weight\", \"model.layers.13.input_layernorm.weight\", \"model.layers.13.post_attention_layernorm.weight\", \"model.layers.14.self_attn.q_proj.weight\", \"model.layers.14.self_attn.k_proj.weight\", \"model.layers.14.self_attn.v_proj.weight\", \"model.layers.14.self_attn.o_proj.weight\", \"model.layers.14.mlp.gate_proj.weight\", \"model.layers.14.mlp.up_proj.weight\", \"model.layers.14.mlp.down_proj.weight\", \"model.layers.14.input_layernorm.weight\", \"model.layers.14.post_attention_layernorm.weight\", \"model.layers.15.self_attn.q_proj.weight\", \"model.layers.15.self_attn.k_proj.weight\", \"model.layers.15.self_attn.v_proj.weight\", \"model.layers.15.self_attn.o_proj.weight\", \"model.layers.15.mlp.gate_proj.weight\", \"model.layers.15.mlp.up_proj.weight\", \"model.layers.15.mlp.down_proj.weight\", \"model.layers.15.input_layernorm.weight\", \"model.layers.15.post_attention_layernorm.weight\", \"model.layers.16.self_attn.q_proj.weight\", \"model.layers.16.self_attn.k_proj.weight\", \"model.layers.16.self_attn.v_proj.weight\", \"model.layers.16.self_attn.o_proj.weight\", \"model.layers.16.mlp.gate_proj.weight\", \"model.layers.16.mlp.up_proj.weight\", \"model.layers.16.mlp.down_proj.weight\", \"model.layers.16.input_layernorm.weight\", \"model.layers.16.post_attention_layernorm.weight\", \"model.layers.17.self_attn.q_proj.weight\", \"model.layers.17.self_attn.k_proj.weight\", \"model.layers.17.self_attn.v_proj.weight\", \"model.layers.17.self_attn.o_proj.weight\", \"model.layers.17.mlp.gate_proj.weight\", \"model.layers.17.mlp.up_proj.weight\", \"model.layers.17.mlp.down_proj.weight\", \"model.layers.17.input_layernorm.weight\", \"model.layers.17.post_attention_layernorm.weight\", \"model.layers.18.self_attn.q_proj.weight\", \"model.layers.18.self_attn.k_proj.weight\", \"model.layers.18.self_attn.v_proj.weight\", \"model.layers.18.self_attn.o_proj.weight\", \"model.layers.18.mlp.gate_proj.weight\", \"model.layers.18.mlp.up_proj.weight\", \"model.layers.18.mlp.down_proj.weight\", \"model.layers.18.input_layernorm.weight\", \"model.layers.18.post_attention_layernorm.weight\", \"model.layers.19.self_attn.q_proj.weight\", \"model.layers.19.self_attn.k_proj.weight\", \"model.layers.19.self_attn.v_proj.weight\", \"model.layers.19.self_attn.o_proj.weight\", \"model.layers.19.mlp.gate_proj.weight\", \"model.layers.19.mlp.up_proj.weight\", \"model.layers.19.mlp.down_proj.weight\", \"model.layers.19.input_layernorm.weight\", \"model.layers.19.post_attention_layernorm.weight\", \"model.layers.20.self_attn.q_proj.weight\", \"model.layers.20.self_attn.k_proj.weight\", \"model.layers.20.self_attn.v_proj.weight\", \"model.layers.20.self_attn.o_proj.weight\", \"model.layers.20.mlp.gate_proj.weight\", \"model.layers.20.mlp.up_proj.weight\", \"model.layers.20.mlp.down_proj.weight\", \"model.layers.20.input_layernorm.weight\", \"model.layers.20.post_attention_layernorm.weight\", \"model.layers.21.self_attn.q_proj.weight\", \"model.layers.21.self_attn.k_proj.weight\", \"model.layers.21.self_attn.v_proj.weight\", \"model.layers.21.self_attn.o_proj.weight\", \"model.layers.21.mlp.gate_proj.weight\", \"model.layers.21.mlp.up_proj.weight\", \"model.layers.21.mlp.down_proj.weight\", \"model.layers.21.input_layernorm.weight\", \"model.layers.21.post_attention_layernorm.weight\", \"model.layers.22.self_attn.q_proj.weight\", \"model.layers.22.self_attn.k_proj.weight\", \"model.layers.22.self_attn.v_proj.weight\", \"model.layers.22.self_attn.o_proj.weight\", \"model.layers.22.mlp.gate_proj.weight\", \"model.layers.22.mlp.up_proj.weight\", \"model.layers.22.mlp.down_proj.weight\", \"model.layers.22.input_layernorm.weight\", \"model.layers.22.post_attention_layernorm.weight\", \"model.layers.23.self_attn.q_proj.weight\", \"model.layers.23.self_attn.k_proj.weight\", \"model.layers.23.self_attn.v_proj.weight\", \"model.layers.23.self_attn.o_proj.weight\", \"model.layers.23.mlp.gate_proj.weight\", \"model.layers.23.mlp.up_proj.weight\", \"model.layers.23.mlp.down_proj.weight\", \"model.layers.23.input_layernorm.weight\", \"model.layers.23.post_attention_layernorm.weight\", \"model.layers.24.self_attn.q_proj.weight\", \"model.layers.24.self_attn.k_proj.weight\", \"model.layers.24.self_attn.v_proj.weight\", \"model.layers.24.self_attn.o_proj.weight\", \"model.layers.24.mlp.gate_proj.weight\", \"model.layers.24.mlp.up_proj.weight\", \"model.layers.24.mlp.down_proj.weight\", \"model.layers.24.input_layernorm.weight\", \"model.layers.24.post_attention_layernorm.weight\", \"model.layers.25.self_attn.q_proj.weight\", \"model.layers.25.self_attn.k_proj.weight\", \"model.layers.25.self_attn.v_proj.weight\", \"model.layers.25.self_attn.o_proj.weight\", \"model.layers.25.mlp.gate_proj.weight\", \"model.layers.25.mlp.up_proj.weight\", \"model.layers.25.mlp.down_proj.weight\", \"model.layers.25.input_layernorm.weight\", \"model.layers.25.post_attention_layernorm.weight\", \"model.layers.26.self_attn.q_proj.weight\", \"model.layers.26.self_attn.k_proj.weight\", \"model.layers.26.self_attn.v_proj.weight\", \"model.layers.26.self_attn.o_proj.weight\", \"model.layers.26.mlp.gate_proj.weight\", \"model.layers.26.mlp.up_proj.weight\", \"model.layers.26.mlp.down_proj.weight\", \"model.layers.26.input_layernorm.weight\", \"model.layers.26.post_attention_layernorm.weight\", \"model.layers.27.self_attn.q_proj.weight\", \"model.layers.27.self_attn.k_proj.weight\", \"model.layers.27.self_attn.v_proj.weight\", \"model.layers.27.self_attn.o_proj.weight\", \"model.layers.27.mlp.gate_proj.weight\", \"model.layers.27.mlp.up_proj.weight\", \"model.layers.27.mlp.down_proj.weight\", \"model.layers.27.input_layernorm.weight\", \"model.layers.27.post_attention_layernorm.weight\", \"model.layers.28.self_attn.q_proj.weight\", \"model.layers.28.self_attn.k_proj.weight\", \"model.layers.28.self_attn.v_proj.weight\", \"model.layers.28.self_attn.o_proj.weight\", \"model.layers.28.mlp.gate_proj.weight\", \"model.layers.28.mlp.up_proj.weight\", \"model.layers.28.mlp.down_proj.weight\", \"model.layers.28.input_layernorm.weight\", \"model.layers.28.post_attention_layernorm.weight\", \"model.layers.29.self_attn.q_proj.weight\", \"model.layers.29.self_attn.k_proj.weight\", \"model.layers.29.self_attn.v_proj.weight\", \"model.layers.29.self_attn.o_proj.weight\", \"model.layers.29.mlp.gate_proj.weight\", \"model.layers.29.mlp.up_proj.weight\", \"model.layers.29.mlp.down_proj.weight\", \"model.layers.29.input_layernorm.weight\", \"model.layers.29.post_attention_layernorm.weight\", \"model.layers.30.self_attn.q_proj.weight\", \"model.layers.30.self_attn.k_proj.weight\", \"model.layers.30.self_attn.v_proj.weight\", \"model.layers.30.self_attn.o_proj.weight\", \"model.layers.30.mlp.gate_proj.weight\", \"model.layers.30.mlp.up_proj.weight\", \"model.layers.30.mlp.down_proj.weight\", \"model.layers.30.input_layernorm.weight\", \"model.layers.30.post_attention_layernorm.weight\", \"model.layers.31.self_attn.q_proj.weight\", \"model.layers.31.self_attn.k_proj.weight\", \"model.layers.31.self_attn.v_proj.weight\", \"model.layers.31.self_attn.o_proj.weight\", \"model.layers.31.mlp.gate_proj.weight\", \"model.layers.31.mlp.up_proj.weight\", \"model.layers.31.mlp.down_proj.weight\", \"model.layers.31.input_layernorm.weight\", \"model.layers.31.post_attention_layernorm.weight\", \"model.norm.weight\", \"lm_head.weight\". \n\tUnexpected key(s) in state_dict: \"tok_embeddings.weight\", \"norm.weight\", \"output.weight\", \"layers.0.attention.wq.weight\", \"layers.0.attention.wk.weight\", \"layers.0.attention.wv.weight\", \"layers.0.attention.wo.weight\", \"layers.0.feed_forward.w1.weight\", \"layers.0.feed_forward.w2.weight\", \"layers.0.feed_forward.w3.weight\", \"layers.0.attention_norm.weight\", \"layers.0.ffn_norm.weight\", \"layers.1.attention.wq.weight\", \"layers.1.attention.wk.weight\", \"layers.1.attention.wv.weight\", \"layers.1.attention.wo.weight\", \"layers.1.feed_forward.w1.weight\", \"layers.1.feed_forward.w2.weight\", \"layers.1.feed_forward.w3.weight\", \"layers.1.attention_norm.weight\", \"layers.1.ffn_norm.weight\", \"layers.2.attention.wq.weight\", \"layers.2.attention.wk.weight\", \"layers.2.attention.wv.weight\", \"layers.2.attention.wo.weight\", \"layers.2.feed_forward.w1.weight\", \"layers.2.feed_forward.w2.weight\", \"layers.2.feed_forward.w3.weight\", \"layers.2.attention_norm.weight\", \"layers.2.ffn_norm.weight\", \"layers.3.attention.wq.weight\", \"layers.3.attention.wk.weight\", \"layers.3.attention.wv.weight\", \"layers.3.attention.wo.weight\", \"layers.3.feed_forward.w1.weight\", \"layers.3.feed_forward.w2.weight\", \"layers.3.feed_forward.w3.weight\", \"layers.3.attention_norm.weight\", \"layers.3.ffn_norm.weight\", \"layers.4.attention.wq.weight\", \"layers.4.attention.wk.weight\", \"layers.4.attention.wv.weight\", \"layers.4.attention.wo.weight\", \"layers.4.feed_forward.w1.weight\", \"layers.4.feed_forward.w2.weight\", \"layers.4.feed_forward.w3.weight\", \"layers.4.attention_norm.weight\", \"layers.4.ffn_norm.weight\", \"layers.5.attention.wq.weight\", \"layers.5.attention.wk.weight\", \"layers.5.attention.wv.weight\", \"layers.5.attention.wo.weight\", \"layers.5.feed_forward.w1.weight\", \"layers.5.feed_forward.w2.weight\", \"layers.5.feed_forward.w3.weight\", \"layers.5.attention_norm.weight\", \"layers.5.ffn_norm.weight\", \"layers.6.attention.wq.weight\", \"layers.6.attention.wk.weight\", \"layers.6.attention.wv.weight\", \"layers.6.attention.wo.weight\", \"layers.6.feed_forward.w1.weight\", \"layers.6.feed_forward.w2.weight\", \"layers.6.feed_forward.w3.weight\", \"layers.6.attention_norm.weight\", \"layers.6.ffn_norm.weight\", \"layers.7.attention.wq.weight\", \"layers.7.attention.wk.weight\", \"layers.7.attention.wv.weight\", \"layers.7.attention.wo.weight\", \"layers.7.feed_forward.w1.weight\", \"layers.7.feed_forward.w2.weight\", \"layers.7.feed_forward.w3.weight\", \"layers.7.attention_norm.weight\", \"layers.7.ffn_norm.weight\", \"layers.8.attention.wq.weight\", \"layers.8.attention.wk.weight\", \"layers.8.attention.wv.weight\", \"layers.8.attention.wo.weight\", \"layers.8.feed_forward.w1.weight\", \"layers.8.feed_forward.w2.weight\", \"layers.8.feed_forward.w3.weight\", \"layers.8.attention_norm.weight\", \"layers.8.ffn_norm.weight\", \"layers.9.attention.wq.weight\", \"layers.9.attention.wk.weight\", \"layers.9.attention.wv.weight\", \"layers.9.attention.wo.weight\", \"layers.9.feed_forward.w1.weight\", \"layers.9.feed_forward.w2.weight\", \"layers.9.feed_forward.w3.weight\", \"layers.9.attention_norm.weight\", \"layers.9.ffn_norm.weight\", \"layers.10.attention.wq.weight\", \"layers.10.attention.wk.weight\", \"layers.10.attention.wv.weight\", \"layers.10.attention.wo.weight\", \"layers.10.feed_forward.w1.weight\", \"layers.10.feed_forward.w2.weight\", \"layers.10.feed_forward.w3.weight\", \"layers.10.attention_norm.weight\", \"layers.10.ffn_norm.weight\", \"layers.11.attention.wq.weight\", \"layers.11.attention.wk.weight\", \"layers.11.attention.wv.weight\", \"layers.11.attention.wo.weight\", \"layers.11.feed_forward.w1.weight\", \"layers.11.feed_forward.w2.weight\", \"layers.11.feed_forward.w3.weight\", \"layers.11.attention_norm.weight\", \"layers.11.ffn_norm.weight\", \"layers.12.attention.wq.weight\", \"layers.12.attention.wk.weight\", \"layers.12.attention.wv.weight\", \"layers.12.attention.wo.weight\", \"layers.12.feed_forward.w1.weight\", \"layers.12.feed_forward.w2.weight\", \"layers.12.feed_forward.w3.weight\", \"layers.12.attention_norm.weight\", \"layers.12.ffn_norm.weight\", \"layers.13.attention.wq.weight\", \"layers.13.attention.wk.weight\", \"layers.13.attention.wv.weight\", \"layers.13.attention.wo.weight\", \"layers.13.feed_forward.w1.weight\", \"layers.13.feed_forward.w2.weight\", \"layers.13.feed_forward.w3.weight\", \"layers.13.attention_norm.weight\", \"layers.13.ffn_norm.weight\", \"layers.14.attention.wq.weight\", \"layers.14.attention.wk.weight\", \"layers.14.attention.wv.weight\", \"layers.14.attention.wo.weight\", \"layers.14.feed_forward.w1.weight\", \"layers.14.feed_forward.w2.weight\", \"layers.14.feed_forward.w3.weight\", \"layers.14.attention_norm.weight\", \"layers.14.ffn_norm.weight\", \"layers.15.attention.wq.weight\", \"layers.15.attention.wk.weight\", \"layers.15.attention.wv.weight\", \"layers.15.attention.wo.weight\", \"layers.15.feed_forward.w1.weight\", \"layers.15.feed_forward.w2.weight\", \"layers.15.feed_forward.w3.weight\", \"layers.15.attention_norm.weight\", \"layers.15.ffn_norm.weight\", \"layers.16.attention.wq.weight\", \"layers.16.attention.wk.weight\", \"layers.16.attention.wv.weight\", \"layers.16.attention.wo.weight\", \"layers.16.feed_forward.w1.weight\", \"layers.16.feed_forward.w2.weight\", \"layers.16.feed_forward.w3.weight\", \"layers.16.attention_norm.weight\", \"layers.16.ffn_norm.weight\", \"layers.17.attention.wq.weight\", \"layers.17.attention.wk.weight\", \"layers.17.attention.wv.weight\", \"layers.17.attention.wo.weight\", \"layers.17.feed_forward.w1.weight\", \"layers.17.feed_forward.w2.weight\", \"layers.17.feed_forward.w3.weight\", \"layers.17.attention_norm.weight\", \"layers.17.ffn_norm.weight\", \"layers.18.attention.wq.weight\", \"layers.18.attention.wk.weight\", \"layers.18.attention.wv.weight\", \"layers.18.attention.wo.weight\", \"layers.18.feed_forward.w1.weight\", \"layers.18.feed_forward.w2.weight\", \"layers.18.feed_forward.w3.weight\", \"layers.18.attention_norm.weight\", \"layers.18.ffn_norm.weight\", \"layers.19.attention.wq.weight\", \"layers.19.attention.wk.weight\", \"layers.19.attention.wv.weight\", \"layers.19.attention.wo.weight\", \"layers.19.feed_forward.w1.weight\", \"layers.19.feed_forward.w2.weight\", \"layers.19.feed_forward.w3.weight\", \"layers.19.attention_norm.weight\", \"layers.19.ffn_norm.weight\", \"layers.20.attention.wq.weight\", \"layers.20.attention.wk.weight\", \"layers.20.attention.wv.weight\", \"layers.20.attention.wo.weight\", \"layers.20.feed_forward.w1.weight\", \"layers.20.feed_forward.w2.weight\", \"layers.20.feed_forward.w3.weight\", \"layers.20.attention_norm.weight\", \"layers.20.ffn_norm.weight\", \"layers.21.attention.wq.weight\", \"layers.21.attention.wk.weight\", \"layers.21.attention.wv.weight\", \"layers.21.attention.wo.weight\", \"layers.21.feed_forward.w1.weight\", \"layers.21.feed_forward.w2.weight\", \"layers.21.feed_forward.w3.weight\", \"layers.21.attention_norm.weight\", \"layers.21.ffn_norm.weight\", \"layers.22.attention.wq.weight\", \"layers.22.attention.wk.weight\", \"layers.22.attention.wv.weight\", \"layers.22.attention.wo.weight\", \"layers.22.feed_forward.w1.weight\", \"layers.22.feed_forward.w2.weight\", \"layers.22.feed_forward.w3.weight\", \"layers.22.attention_norm.weight\", \"layers.22.ffn_norm.weight\", \"layers.23.attention.wq.weight\", \"layers.23.attention.wk.weight\", \"layers.23.attention.wv.weight\", \"layers.23.attention.wo.weight\", \"layers.23.feed_forward.w1.weight\", \"layers.23.feed_forward.w2.weight\", \"layers.23.feed_forward.w3.weight\", \"layers.23.attention_norm.weight\", \"layers.23.ffn_norm.weight\", \"layers.24.attention.wq.weight\", \"layers.24.attention.wk.weight\", \"layers.24.attention.wv.weight\", \"layers.24.attention.wo.weight\", \"layers.24.feed_forward.w1.weight\", \"layers.24.feed_forward.w2.weight\", \"layers.24.feed_forward.w3.weight\", \"layers.24.attention_norm.weight\", \"layers.24.ffn_norm.weight\", \"layers.25.attention.wq.weight\", \"layers.25.attention.wk.weight\", \"layers.25.attention.wv.weight\", \"layers.25.attention.wo.weight\", \"layers.25.feed_forward.w1.weight\", \"layers.25.feed_forward.w2.weight\", \"layers.25.feed_forward.w3.weight\", \"layers.25.attention_norm.weight\", \"layers.25.ffn_norm.weight\", \"layers.26.attention.wq.weight\", \"layers.26.attention.wk.weight\", \"layers.26.attention.wv.weight\", \"layers.26.attention.wo.weight\", \"layers.26.feed_forward.w1.weight\", \"layers.26.feed_forward.w2.weight\", \"layers.26.feed_forward.w3.weight\", \"layers.26.attention_norm.weight\", \"layers.26.ffn_norm.weight\", \"layers.27.attention.wq.weight\", \"layers.27.attention.wk.weight\", \"layers.27.attention.wv.weight\", \"layers.27.attention.wo.weight\", \"layers.27.feed_forward.w1.weight\", \"layers.27.feed_forward.w2.weight\", \"layers.27.feed_forward.w3.weight\", \"layers.27.attention_norm.weight\", \"layers.27.ffn_norm.weight\", \"layers.28.attention.wq.weight\", \"layers.28.attention.wk.weight\", \"layers.28.attention.wv.weight\", \"layers.28.attention.wo.weight\", \"layers.28.feed_forward.w1.weight\", \"layers.28.feed_forward.w2.weight\", \"layers.28.feed_forward.w3.weight\", \"layers.28.attention_norm.weight\", \"layers.28.ffn_norm.weight\", \"layers.29.attention.wq.weight\", \"layers.29.attention.wk.weight\", \"layers.29.attention.wv.weight\", \"layers.29.attention.wo.weight\", \"layers.29.feed_forward.w1.weight\", \"layers.29.feed_forward.w2.weight\", \"layers.29.feed_forward.w3.weight\", \"layers.29.attention_norm.weight\", \"layers.29.ffn_norm.weight\", \"layers.30.attention.wq.weight\", \"layers.30.attention.wk.weight\", \"layers.30.attention.wv.weight\", \"layers.30.attention.wo.weight\", \"layers.30.feed_forward.w1.weight\", \"layers.30.feed_forward.w2.weight\", \"layers.30.feed_forward.w3.weight\", \"layers.30.attention_norm.weight\", \"layers.30.ffn_norm.weight\", \"layers.31.attention.wq.weight\", \"layers.31.attention.wk.weight\", \"layers.31.attention.wv.weight\", \"layers.31.attention.wo.weight\", \"layers.31.feed_forward.w1.weight\", \"layers.31.feed_forward.w2.weight\", \"layers.31.feed_forward.w3.weight\", \"layers.31.attention_norm.weight\", \"layers.31.ffn_norm.weight\", \"rope.freqs\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m LlamaForCausalLM(config)\n\u001b[1;32m     12\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/shared/4/models/llama2/llama-2-7b/consolidated.00.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for LlamaForCausalLM:\n\tMissing key(s) in state_dict: \"model.embed_tokens.weight\", \"model.layers.0.self_attn.q_proj.weight\", \"model.layers.0.self_attn.k_proj.weight\", \"model.layers.0.self_attn.v_proj.weight\", \"model.layers.0.self_attn.o_proj.weight\", \"model.layers.0.mlp.gate_proj.weight\", \"model.layers.0.mlp.up_proj.weight\", \"model.layers.0.mlp.down_proj.weight\", \"model.layers.0.input_layernorm.weight\", \"model.layers.0.post_attention_layernorm.weight\", \"model.layers.1.self_attn.q_proj.weight\", \"model.layers.1.self_attn.k_proj.weight\", \"model.layers.1.self_attn.v_proj.weight\", \"model.layers.1.self_attn.o_proj.weight\", \"model.layers.1.mlp.gate_proj.weight\", \"model.layers.1.mlp.up_proj.weight\", \"model.layers.1.mlp.down_proj.weight\", \"model.layers.1.input_layernorm.weight\", \"model.layers.1.post_attention_layernorm.weight\", \"model.layers.2.self_attn.q_proj.weight\", \"model.layers.2.self_attn.k_proj.weight\", \"model.layers.2.self_attn.v_proj.weight\", \"model.layers.2.self_attn.o_proj.weight\", \"model.layers.2.mlp.gate_proj.weight\", \"model.layers.2.mlp.up_proj.weight\", \"model.layers.2.mlp.down_proj.weight\", \"model.layers.2.input_layernorm.weight\", \"model.layers.2.post_attention_layernorm.weight\", \"model.layers.3.self_attn.q_proj.weight\", \"model.layers.3.self_attn.k_proj.weight\", \"model.layers.3.self_attn.v_proj.weight\", \"model.layers.3.self_attn.o_proj.weight\", \"model.layers.3.mlp.gate_proj.weight\", \"model.layers.3.mlp.up_proj.weight\", \"model.layers.3.mlp.down_proj.weight\", \"model.layers.3.input_layernorm.weight\", \"model.layers.3.post_attention_layernorm.weight\", \"model.layers.4.self_attn.q_proj.weight\", \"model.layers.4.self_attn.k_proj.weight\", \"model.layers.4.self_attn.v_proj.weight\", \"model.layers.4.self_attn.o_proj.weight\", \"model.layers.4.mlp.gate_proj.weight\", \"model.layers.4.mlp.up_proj.weight\", \"model.layers.4.mlp.down_proj.weight\", \"model.layers.4.input_layernorm.weight\", \"model.layers.4.post_attention_layernorm.weight\", \"model.layers.5.self_attn.q_proj.weight\", \"model.layers.5.self_attn.k_proj.weight\", \"model.layers.5.self_attn.v_proj.weight\", \"model.layers.5.self_attn.o_proj.weight\", \"model.layers.5.mlp.gate_proj.weight\", \"model.layers.5.mlp.up_proj.weight\", \"model.layers.5.mlp.down_proj.weight\", \"model.layers.5.input_layernorm.weight\", \"model.layers.5.post_attention_layernorm.weight\", \"model.layers.6.self_attn.q_proj.weight\", \"model.layers.6.self_attn.k_proj.weight\", \"model.layers.6.self_attn.v_proj.weight\", \"model.layers.6.self_attn.o_proj.weight\", \"model.layers.6.mlp.gate_proj.weight\", \"model.layers.6.mlp.up_proj.weight\", \"model.layers.6.mlp.down_proj.weight\", \"model.layers.6.input_layernorm.weight\", \"model.layers.6.post_attention_layernorm.weight\", \"model.layers.7.self_attn.q_proj.weight\", \"model.layers.7.self_attn.k_proj.weight\", \"model.layers.7.self_attn.v_proj.weight\", \"model.layers.7.self_attn.o_proj.weight\", \"model.layers.7.mlp.gate_proj.weight\", \"model.layers.7.mlp.up_proj.weight\", \"model.layers.7.mlp.down_proj.weight\", \"model.layers.7.input_layernorm.weight\", \"model.layers.7.post_attention_layernorm.weight\", \"model.layers.8.self_attn.q_proj.weight\", \"model.layers.8.self_attn.k_proj.weight\", \"model.layers.8.self_attn.v_proj.weight\", \"model.layers.8.self_attn.o_proj.weight\", \"model.layers.8.mlp.gate_proj.weight\", \"model.layers.8.mlp.up_proj.weight\", \"model.layers.8.mlp.down_proj.weight\", \"model.layers.8.input_layernorm.weight\", \"model.layers.8.post_attention_layernorm.weight\", \"model.layers.9.self_attn.q_proj.weight\", \"model.layers.9.self_attn.k_proj.weight\", \"model.layers.9.self_attn.v_proj.weight\", \"model.layers.9.self_attn.o_proj.weight\", \"model.layers.9.mlp.gate_proj.weight\", \"model.layers.9.mlp.up_proj.weight\", \"model.layers.9.mlp.down_proj.weight\", \"model.layers.9.input_layernorm.weight\", \"model.layers.9.post_attention_layernorm.weight\", \"model.layers.10.self_attn.q_proj.weight\", \"model.layers.10.self_attn.k_proj.weight\", \"model.layers.10.self_attn.v_proj.weight\", \"model.layers.10.self_attn.o_proj.weight\", \"model.layers.10.mlp.gate_proj.weight\", \"model.layers.10.mlp.up_proj.weight\", \"model.layers.10.mlp.down_proj.weight\", \"model.layers.10.input_layernorm.weight\", \"model.layers.10.post_attention_layernorm.weight\", \"model.layers.11.self_attn.q_proj.weight\", \"model.layers.11.self_attn.k_proj.weight\", \"model.layers.11.self_attn.v_proj.weight\", \"model.layers.11.self_attn.o_proj.weight\", \"model.layers.11.mlp.gate_proj.weight\", \"model.layers.11.mlp.up_proj.weight\", \"model.layers.11.mlp.down_proj.weight\", \"model.layers.11.input_layernorm.weight\", \"model.layers.11.post_attention_layernorm.weight\", \"model.layers.12.self_attn.q_proj.weight\", \"model.layers.12.self_attn.k_proj.weight\", \"model.layers.12.self_attn.v_proj.weight\", \"model.layers.12.self_attn.o_proj.weight\", \"model.layers.12.mlp.gate_proj.weight\", \"model.layers.12.mlp.up_proj.weight\", \"model.layers.12.mlp.down_proj.weight\", \"model.layers.12.input_layernorm.weight\", \"model.layers.12.post_attention_layernorm.weight\", \"model.layers.13.self_attn.q_proj.weight\", \"model.layers.13.self_attn.k_proj.weight\", \"model.layers.13.self_attn.v_proj.weight\", \"model.layers.13.self_attn.o_proj.weight\", \"model.layers.13.mlp.gate_proj.weight\", \"model.layers.13.mlp.up_proj.weight\", \"model.layers.13.mlp.down_proj.weight\", \"model.layers.13.input_layernorm.weight\", \"model.layers.13.post_attention_layernorm.weight\", \"model.layers.14.self_attn.q_proj.weight\", \"model.layers.14.self_attn.k_proj.weight\", \"model.layers.14.self_attn.v_proj.weight\", \"model.layers.14.self_attn.o_proj.weight\", \"model.layers.14.mlp.gate_proj.weight\", \"model.layers.14.mlp.up_proj.weight\", \"model.layers.14.mlp.down_proj.weight\", \"model.layers.14.input_layernorm.weight\", \"model.layers.14.post_attention_layernorm.weight\", \"model.layers.15.self_attn.q_proj.weight\", \"model.layers.15.self_attn.k_proj.weight\", \"model.layers.15.self_attn.v_proj.weight\", \"model.layers.15.self_attn.o_proj.weight\", \"model.layers.15.mlp.gate_proj.weight\", \"model.layers.15.mlp.up_proj.weight\", \"model.layers.15.mlp.down_proj.weight\", \"model.layers.15.input_layernorm.weight\", \"model.layers.15.post_attention_layernorm.weight\", \"model.layers.16.self_attn.q_proj.weight\", \"model.layers.16.self_attn.k_proj.weight\", \"model.layers.16.self_attn.v_proj.weight\", \"model.layers.16.self_attn.o_proj.weight\", \"model.layers.16.mlp.gate_proj.weight\", \"model.layers.16.mlp.up_proj.weight\", \"model.layers.16.mlp.down_proj.weight\", \"model.layers.16.input_layernorm.weight\", \"model.layers.16.post_attention_layernorm.weight\", \"model.layers.17.self_attn.q_proj.weight\", \"model.layers.17.self_attn.k_proj.weight\", \"model.layers.17.self_attn.v_proj.weight\", \"model.layers.17.self_attn.o_proj.weight\", \"model.layers.17.mlp.gate_proj.weight\", \"model.layers.17.mlp.up_proj.weight\", \"model.layers.17.mlp.down_proj.weight\", \"model.layers.17.input_layernorm.weight\", \"model.layers.17.post_attention_layernorm.weight\", \"model.layers.18.self_attn.q_proj.weight\", \"model.layers.18.self_attn.k_proj.weight\", \"model.layers.18.self_attn.v_proj.weight\", \"model.layers.18.self_attn.o_proj.weight\", \"model.layers.18.mlp.gate_proj.weight\", \"model.layers.18.mlp.up_proj.weight\", \"model.layers.18.mlp.down_proj.weight\", \"model.layers.18.input_layernorm.weight\", \"model.layers.18.post_attention_layernorm.weight\", \"model.layers.19.self_attn.q_proj.weight\", \"model.layers.19.self_attn.k_proj.weight\", \"model.layers.19.self_attn.v_proj.weight\", \"model.layers.19.self_attn.o_proj.weight\", \"model.layers.19.mlp.gate_proj.weight\", \"model.layers.19.mlp.up_proj.weight\", \"model.layers.19.mlp.down_proj.weight\", \"model.layers.19.input_layernorm.weight\", \"model.layers.19.post_attention_layernorm.weight\", \"model.layers.20.self_attn.q_proj.weight\", \"model.layers.20.self_attn.k_proj.weight\", \"model.layers.20.self_attn.v_proj.weight\", \"model.layers.20.self_attn.o_proj.weight\", \"model.layers.20.mlp.gate_proj.weight\", \"model.layers.20.mlp.up_proj.weight\", \"model.layers.20.mlp.down_proj.weight\", \"model.layers.20.input_layernorm.weight\", \"model.layers.20.post_attention_layernorm.weight\", \"model.layers.21.self_attn.q_proj.weight\", \"model.layers.21.self_attn.k_proj.weight\", \"model.layers.21.self_attn.v_proj.weight\", \"model.layers.21.self_attn.o_proj.weight\", \"model.layers.21.mlp.gate_proj.weight\", \"model.layers.21.mlp.up_proj.weight\", \"model.layers.21.mlp.down_proj.weight\", \"model.layers.21.input_layernorm.weight\", \"model.layers.21.post_attention_layernorm.weight\", \"model.layers.22.self_attn.q_proj.weight\", \"model.layers.22.self_attn.k_proj.weight\", \"model.layers.22.self_attn.v_proj.weight\", \"model.layers.22.self_attn.o_proj.weight\", \"model.layers.22.mlp.gate_proj.weight\", \"model.layers.22.mlp.up_proj.weight\", \"model.layers.22.mlp.down_proj.weight\", \"model.layers.22.input_layernorm.weight\", \"model.layers.22.post_attention_layernorm.weight\", \"model.layers.23.self_attn.q_proj.weight\", \"model.layers.23.self_attn.k_proj.weight\", \"model.layers.23.self_attn.v_proj.weight\", \"model.layers.23.self_attn.o_proj.weight\", \"model.layers.23.mlp.gate_proj.weight\", \"model.layers.23.mlp.up_proj.weight\", \"model.layers.23.mlp.down_proj.weight\", \"model.layers.23.input_layernorm.weight\", \"model.layers.23.post_attention_layernorm.weight\", \"model.layers.24.self_attn.q_proj.weight\", \"model.layers.24.self_attn.k_proj.weight\", \"model.layers.24.self_attn.v_proj.weight\", \"model.layers.24.self_attn.o_proj.weight\", \"model.layers.24.mlp.gate_proj.weight\", \"model.layers.24.mlp.up_proj.weight\", \"model.layers.24.mlp.down_proj.weight\", \"model.layers.24.input_layernorm.weight\", \"model.layers.24.post_attention_layernorm.weight\", \"model.layers.25.self_attn.q_proj.weight\", \"model.layers.25.self_attn.k_proj.weight\", \"model.layers.25.self_attn.v_proj.weight\", \"model.layers.25.self_attn.o_proj.weight\", \"model.layers.25.mlp.gate_proj.weight\", \"model.layers.25.mlp.up_proj.weight\", \"model.layers.25.mlp.down_proj.weight\", \"model.layers.25.input_layernorm.weight\", \"model.layers.25.post_attention_layernorm.weight\", \"model.layers.26.self_attn.q_proj.weight\", \"model.layers.26.self_attn.k_proj.weight\", \"model.layers.26.self_attn.v_proj.weight\", \"model.layers.26.self_attn.o_proj.weight\", \"model.layers.26.mlp.gate_proj.weight\", \"model.layers.26.mlp.up_proj.weight\", \"model.layers.26.mlp.down_proj.weight\", \"model.layers.26.input_layernorm.weight\", \"model.layers.26.post_attention_layernorm.weight\", \"model.layers.27.self_attn.q_proj.weight\", \"model.layers.27.self_attn.k_proj.weight\", \"model.layers.27.self_attn.v_proj.weight\", \"model.layers.27.self_attn.o_proj.weight\", \"model.layers.27.mlp.gate_proj.weight\", \"model.layers.27.mlp.up_proj.weight\", \"model.layers.27.mlp.down_proj.weight\", \"model.layers.27.input_layernorm.weight\", \"model.layers.27.post_attention_layernorm.weight\", \"model.layers.28.self_attn.q_proj.weight\", \"model.layers.28.self_attn.k_proj.weight\", \"model.layers.28.self_attn.v_proj.weight\", \"model.layers.28.self_attn.o_proj.weight\", \"model.layers.28.mlp.gate_proj.weight\", \"model.layers.28.mlp.up_proj.weight\", \"model.layers.28.mlp.down_proj.weight\", \"model.layers.28.input_layernorm.weight\", \"model.layers.28.post_attention_layernorm.weight\", \"model.layers.29.self_attn.q_proj.weight\", \"model.layers.29.self_attn.k_proj.weight\", \"model.layers.29.self_attn.v_proj.weight\", \"model.layers.29.self_attn.o_proj.weight\", \"model.layers.29.mlp.gate_proj.weight\", \"model.layers.29.mlp.up_proj.weight\", \"model.layers.29.mlp.down_proj.weight\", \"model.layers.29.input_layernorm.weight\", \"model.layers.29.post_attention_layernorm.weight\", \"model.layers.30.self_attn.q_proj.weight\", \"model.layers.30.self_attn.k_proj.weight\", \"model.layers.30.self_attn.v_proj.weight\", \"model.layers.30.self_attn.o_proj.weight\", \"model.layers.30.mlp.gate_proj.weight\", \"model.layers.30.mlp.up_proj.weight\", \"model.layers.30.mlp.down_proj.weight\", \"model.layers.30.input_layernorm.weight\", \"model.layers.30.post_attention_layernorm.weight\", \"model.layers.31.self_attn.q_proj.weight\", \"model.layers.31.self_attn.k_proj.weight\", \"model.layers.31.self_attn.v_proj.weight\", \"model.layers.31.self_attn.o_proj.weight\", \"model.layers.31.mlp.gate_proj.weight\", \"model.layers.31.mlp.up_proj.weight\", \"model.layers.31.mlp.down_proj.weight\", \"model.layers.31.input_layernorm.weight\", \"model.layers.31.post_attention_layernorm.weight\", \"model.norm.weight\", \"lm_head.weight\". \n\tUnexpected key(s) in state_dict: \"tok_embeddings.weight\", \"norm.weight\", \"output.weight\", \"layers.0.attention.wq.weight\", \"layers.0.attention.wk.weight\", \"layers.0.attention.wv.weight\", \"layers.0.attention.wo.weight\", \"layers.0.feed_forward.w1.weight\", \"layers.0.feed_forward.w2.weight\", \"layers.0.feed_forward.w3.weight\", \"layers.0.attention_norm.weight\", \"layers.0.ffn_norm.weight\", \"layers.1.attention.wq.weight\", \"layers.1.attention.wk.weight\", \"layers.1.attention.wv.weight\", \"layers.1.attention.wo.weight\", \"layers.1.feed_forward.w1.weight\", \"layers.1.feed_forward.w2.weight\", \"layers.1.feed_forward.w3.weight\", \"layers.1.attention_norm.weight\", \"layers.1.ffn_norm.weight\", \"layers.2.attention.wq.weight\", \"layers.2.attention.wk.weight\", \"layers.2.attention.wv.weight\", \"layers.2.attention.wo.weight\", \"layers.2.feed_forward.w1.weight\", \"layers.2.feed_forward.w2.weight\", \"layers.2.feed_forward.w3.weight\", \"layers.2.attention_norm.weight\", \"layers.2.ffn_norm.weight\", \"layers.3.attention.wq.weight\", \"layers.3.attention.wk.weight\", \"layers.3.attention.wv.weight\", \"layers.3.attention.wo.weight\", \"layers.3.feed_forward.w1.weight\", \"layers.3.feed_forward.w2.weight\", \"layers.3.feed_forward.w3.weight\", \"layers.3.attention_norm.weight\", \"layers.3.ffn_norm.weight\", \"layers.4.attention.wq.weight\", \"layers.4.attention.wk.weight\", \"layers.4.attention.wv.weight\", \"layers.4.attention.wo.weight\", \"layers.4.feed_forward.w1.weight\", \"layers.4.feed_forward.w2.weight\", \"layers.4.feed_forward.w3.weight\", \"layers.4.attention_norm.weight\", \"layers.4.ffn_norm.weight\", \"layers.5.attention.wq.weight\", \"layers.5.attention.wk.weight\", \"layers.5.attention.wv.weight\", \"layers.5.attention.wo.weight\", \"layers.5.feed_forward.w1.weight\", \"layers.5.feed_forward.w2.weight\", \"layers.5.feed_forward.w3.weight\", \"layers.5.attention_norm.weight\", \"layers.5.ffn_norm.weight\", \"layers.6.attention.wq.weight\", \"layers.6.attention.wk.weight\", \"layers.6.attention.wv.weight\", \"layers.6.attention.wo.weight\", \"layers.6.feed_forward.w1.weight\", \"layers.6.feed_forward.w2.weight\", \"layers.6.feed_forward.w3.weight\", \"layers.6.attention_norm.weight\", \"layers.6.ffn_norm.weight\", \"layers.7.attention.wq.weight\", \"layers.7.attention.wk.weight\", \"layers.7.attention.wv.weight\", \"layers.7.attention.wo.weight\", \"layers.7.feed_forward.w1.weight\", \"layers.7.feed_forward.w2.weight\", \"layers.7.feed_forward.w3.weight\", \"layers.7.attention_norm.weight\", \"layers.7.ffn_norm.weight\", \"layers.8.attention.wq.weight\", \"layers.8.attention.wk.weight\", \"layers.8.attention.wv.weight\", \"layers.8.attention.wo.weight\", \"layers.8.feed_forward.w1.weight\", \"layers.8.feed_forward.w2.weight\", \"layers.8.feed_forward.w3.weight\", \"layers.8.attention_norm.weight\", \"layers.8.ffn_norm.weight\", \"layers.9.attention.wq.weight\", \"layers.9.attention.wk.weight\", \"layers.9.attention.wv.weight\", \"layers.9.attention.wo.weight\", \"layers.9.feed_forward.w1.weight\", \"layers.9.feed_forward.w2.weight\", \"layers.9.feed_forward.w3.weight\", \"layers.9.attention_norm.weight\", \"layers.9.ffn_norm.weight\", \"layers.10.attention.wq.weight\", \"layers.10.attention.wk.weight\", \"layers.10.attention.wv.weight\", \"layers.10.attention.wo.weight\", \"layers.10.feed_forward.w1.weight\", \"layers.10.feed_forward.w2.weight\", \"layers.10.feed_forward.w3.weight\", \"layers.10.attention_norm.weight\", \"layers.10.ffn_norm.weight\", \"layers.11.attention.wq.weight\", \"layers.11.attention.wk.weight\", \"layers.11.attention.wv.weight\", \"layers.11.attention.wo.weight\", \"layers.11.feed_forward.w1.weight\", \"layers.11.feed_forward.w2.weight\", \"layers.11.feed_forward.w3.weight\", \"layers.11.attention_norm.weight\", \"layers.11.ffn_norm.weight\", \"layers.12.attention.wq.weight\", \"layers.12.attention.wk.weight\", \"layers.12.attention.wv.weight\", \"layers.12.attention.wo.weight\", \"layers.12.feed_forward.w1.weight\", \"layers.12.feed_forward.w2.weight\", \"layers.12.feed_forward.w3.weight\", \"layers.12.attention_norm.weight\", \"layers.12.ffn_norm.weight\", \"layers.13.attention.wq.weight\", \"layers.13.attention.wk.weight\", \"layers.13.attention.wv.weight\", \"layers.13.attention.wo.weight\", \"layers.13.feed_forward.w1.weight\", \"layers.13.feed_forward.w2.weight\", \"layers.13.feed_forward.w3.weight\", \"layers.13.attention_norm.weight\", \"layers.13.ffn_norm.weight\", \"layers.14.attention.wq.weight\", \"layers.14.attention.wk.weight\", \"layers.14.attention.wv.weight\", \"layers.14.attention.wo.weight\", \"layers.14.feed_forward.w1.weight\", \"layers.14.feed_forward.w2.weight\", \"layers.14.feed_forward.w3.weight\", \"layers.14.attention_norm.weight\", \"layers.14.ffn_norm.weight\", \"layers.15.attention.wq.weight\", \"layers.15.attention.wk.weight\", \"layers.15.attention.wv.weight\", \"layers.15.attention.wo.weight\", \"layers.15.feed_forward.w1.weight\", \"layers.15.feed_forward.w2.weight\", \"layers.15.feed_forward.w3.weight\", \"layers.15.attention_norm.weight\", \"layers.15.ffn_norm.weight\", \"layers.16.attention.wq.weight\", \"layers.16.attention.wk.weight\", \"layers.16.attention.wv.weight\", \"layers.16.attention.wo.weight\", \"layers.16.feed_forward.w1.weight\", \"layers.16.feed_forward.w2.weight\", \"layers.16.feed_forward.w3.weight\", \"layers.16.attention_norm.weight\", \"layers.16.ffn_norm.weight\", \"layers.17.attention.wq.weight\", \"layers.17.attention.wk.weight\", \"layers.17.attention.wv.weight\", \"layers.17.attention.wo.weight\", \"layers.17.feed_forward.w1.weight\", \"layers.17.feed_forward.w2.weight\", \"layers.17.feed_forward.w3.weight\", \"layers.17.attention_norm.weight\", \"layers.17.ffn_norm.weight\", \"layers.18.attention.wq.weight\", \"layers.18.attention.wk.weight\", \"layers.18.attention.wv.weight\", \"layers.18.attention.wo.weight\", \"layers.18.feed_forward.w1.weight\", \"layers.18.feed_forward.w2.weight\", \"layers.18.feed_forward.w3.weight\", \"layers.18.attention_norm.weight\", \"layers.18.ffn_norm.weight\", \"layers.19.attention.wq.weight\", \"layers.19.attention.wk.weight\", \"layers.19.attention.wv.weight\", \"layers.19.attention.wo.weight\", \"layers.19.feed_forward.w1.weight\", \"layers.19.feed_forward.w2.weight\", \"layers.19.feed_forward.w3.weight\", \"layers.19.attention_norm.weight\", \"layers.19.ffn_norm.weight\", \"layers.20.attention.wq.weight\", \"layers.20.attention.wk.weight\", \"layers.20.attention.wv.weight\", \"layers.20.attention.wo.weight\", \"layers.20.feed_forward.w1.weight\", \"layers.20.feed_forward.w2.weight\", \"layers.20.feed_forward.w3.weight\", \"layers.20.attention_norm.weight\", \"layers.20.ffn_norm.weight\", \"layers.21.attention.wq.weight\", \"layers.21.attention.wk.weight\", \"layers.21.attention.wv.weight\", \"layers.21.attention.wo.weight\", \"layers.21.feed_forward.w1.weight\", \"layers.21.feed_forward.w2.weight\", \"layers.21.feed_forward.w3.weight\", \"layers.21.attention_norm.weight\", \"layers.21.ffn_norm.weight\", \"layers.22.attention.wq.weight\", \"layers.22.attention.wk.weight\", \"layers.22.attention.wv.weight\", \"layers.22.attention.wo.weight\", \"layers.22.feed_forward.w1.weight\", \"layers.22.feed_forward.w2.weight\", \"layers.22.feed_forward.w3.weight\", \"layers.22.attention_norm.weight\", \"layers.22.ffn_norm.weight\", \"layers.23.attention.wq.weight\", \"layers.23.attention.wk.weight\", \"layers.23.attention.wv.weight\", \"layers.23.attention.wo.weight\", \"layers.23.feed_forward.w1.weight\", \"layers.23.feed_forward.w2.weight\", \"layers.23.feed_forward.w3.weight\", \"layers.23.attention_norm.weight\", \"layers.23.ffn_norm.weight\", \"layers.24.attention.wq.weight\", \"layers.24.attention.wk.weight\", \"layers.24.attention.wv.weight\", \"layers.24.attention.wo.weight\", \"layers.24.feed_forward.w1.weight\", \"layers.24.feed_forward.w2.weight\", \"layers.24.feed_forward.w3.weight\", \"layers.24.attention_norm.weight\", \"layers.24.ffn_norm.weight\", \"layers.25.attention.wq.weight\", \"layers.25.attention.wk.weight\", \"layers.25.attention.wv.weight\", \"layers.25.attention.wo.weight\", \"layers.25.feed_forward.w1.weight\", \"layers.25.feed_forward.w2.weight\", \"layers.25.feed_forward.w3.weight\", \"layers.25.attention_norm.weight\", \"layers.25.ffn_norm.weight\", \"layers.26.attention.wq.weight\", \"layers.26.attention.wk.weight\", \"layers.26.attention.wv.weight\", \"layers.26.attention.wo.weight\", \"layers.26.feed_forward.w1.weight\", \"layers.26.feed_forward.w2.weight\", \"layers.26.feed_forward.w3.weight\", \"layers.26.attention_norm.weight\", \"layers.26.ffn_norm.weight\", \"layers.27.attention.wq.weight\", \"layers.27.attention.wk.weight\", \"layers.27.attention.wv.weight\", \"layers.27.attention.wo.weight\", \"layers.27.feed_forward.w1.weight\", \"layers.27.feed_forward.w2.weight\", \"layers.27.feed_forward.w3.weight\", \"layers.27.attention_norm.weight\", \"layers.27.ffn_norm.weight\", \"layers.28.attention.wq.weight\", \"layers.28.attention.wk.weight\", \"layers.28.attention.wv.weight\", \"layers.28.attention.wo.weight\", \"layers.28.feed_forward.w1.weight\", \"layers.28.feed_forward.w2.weight\", \"layers.28.feed_forward.w3.weight\", \"layers.28.attention_norm.weight\", \"layers.28.ffn_norm.weight\", \"layers.29.attention.wq.weight\", \"layers.29.attention.wk.weight\", \"layers.29.attention.wv.weight\", \"layers.29.attention.wo.weight\", \"layers.29.feed_forward.w1.weight\", \"layers.29.feed_forward.w2.weight\", \"layers.29.feed_forward.w3.weight\", \"layers.29.attention_norm.weight\", \"layers.29.ffn_norm.weight\", \"layers.30.attention.wq.weight\", \"layers.30.attention.wk.weight\", \"layers.30.attention.wv.weight\", \"layers.30.attention.wo.weight\", \"layers.30.feed_forward.w1.weight\", \"layers.30.feed_forward.w2.weight\", \"layers.30.feed_forward.w3.weight\", \"layers.30.attention_norm.weight\", \"layers.30.ffn_norm.weight\", \"layers.31.attention.wq.weight\", \"layers.31.attention.wk.weight\", \"layers.31.attention.wv.weight\", \"layers.31.attention.wo.weight\", \"layers.31.feed_forward.w1.weight\", \"layers.31.feed_forward.w2.weight\", \"layers.31.feed_forward.w3.weight\", \"layers.31.attention_norm.weight\", \"layers.31.ffn_norm.weight\", \"rope.freqs\". "
     ]
    }
   ],
   "source": [
    "'''\n",
    "# BASE_MODEL = \"decapoda-research/llama-7b-hf\"\n",
    "BASE_MODEL = \"model/llama2-7B-hf\"\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "'''\n",
    "model = LlamaForCausalLM(config)\n",
    "state_dict = torch.load('/shared/4/models/llama2/llama-2-7b/consolidated.00.pth')\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd26af9f-2165-4c97-8429-b9e89ded8902",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "tokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "tokenizer.pad_token_id = (\n",
    "    0  # unk. we want this to be different from the eos token\n",
    ")\n",
    "tokenizer.padding_side = \"left\"\n",
    "'''\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained('/shared/4/models/llama2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1f7b1db-ebdf-4a98-9ab8-73b0b25c9cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'text'],\n",
       "    num_rows: 52002\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = load_dataset(\"json\", data_files=\"data/alpaca-trajectory-dataset-train-3in-1out-4.json\")\n",
    "\n",
    "data = load_dataset(\"vicgalle/alpaca-gpt4\")\n",
    "data[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c0b5445-62ec-4af0-845b-9a852306cbcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Give three tips for staying healthy.',\n",
       " 'input': '',\n",
       " 'output': '1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\\n\\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\\n\\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.',\n",
       " 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGive three tips for staying healthy.\\n\\n### Response:\\n1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\\n\\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\\n\\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3ba455d-84f0-4466-b737-11a41c304add",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUTOFF_LEN=4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29e40081-2b0e-4d05-b091-57b3bd5b69f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.  # noqa: E501\n",
    "### Instruction:\n",
    "{data_point[\"instruction\"]}\n",
    "### Input:\n",
    "{data_point[\"input\"]}\n",
    "### Response:\n",
    "{data_point[\"output\"]}\"\"\"\n",
    "\n",
    "\n",
    "def tokenize(prompt, add_eos_token=True):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=CUTOFF_LEN,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    if (\n",
    "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "        and len(result[\"input_ids\"]) < CUTOFF_LEN\n",
    "        and add_eos_token\n",
    "    ):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = generate_prompt(data_point)\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c785906-4150-44e5-8df9-551525ce95f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val = data[\"train\"].train_test_split(\n",
    "    test_size=0.1, shuffle=True, seed=42\n",
    ")\n",
    "train_data = (\n",
    "    train_val[\"train\"].map(generate_and_tokenize_prompt)\n",
    ")\n",
    "val_data = (\n",
    "    train_val[\"test\"].map(generate_and_tokenize_prompt)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b31e3ed-38b0-4a83-9716-f4d26fffed8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'text'],\n",
       "        num_rows: 46801\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'text'],\n",
       "        num_rows: 5201\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c2c8404-e143-4522-a4bf-7149221e3811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_data[0]['input_ids'])\n",
    "# train_filtered = []\n",
    "# for instance in train_data:\n",
    "#     # print(len(instance['input_ids']))\n",
    "#     if len(instance['input_ids']) < 4096:\n",
    "#         train_filtered.append(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18b7056a-eb57-41c0-97f9-b9d22272f6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_filtered[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "069a540a-50dd-4195-877b-aef820c6f4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = train_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19153d12-5977-4535-b8c9-5135c1ca07ba",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2d5fdf8-ade2-476f-8de8-5b235185e528",
   "metadata": {},
   "outputs": [],
   "source": [
    "LORA_R = 8\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT= 0.05\n",
    "LORA_TARGET_MODULES = [\n",
    "    \"q_proj\",\n",
    "    \"v_proj\",\n",
    "]\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "MICRO_BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
    "LEARNING_RATE = 8e-4\n",
    "TRAIN_STEPS = 10\n",
    "OUTPUT_DIR = \"model/llama-2-7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0f2fc4f-0843-4538-a911-c779d3f911dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = prepare_model_for_int8_training(model)\n",
    "config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=LORA_TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "# model = get_peft_model(model, config)\n",
    "# model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01db86e2-5fd6-418c-8a81-515e6c20f067",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=MICRO_BATCH_SIZE,\n",
    "    # gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    num_train_epochs=3,\n",
    "    # warmup_steps=5,\n",
    "    # max_steps=TRAIN_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    save_strategy='epoch',\n",
    "    evaluation_strategy='epoch',\n",
    "    do_eval=True,\n",
    "    # logging_dir=output_dir + 'logs/',\n",
    "    fp16=True,   # Change it to False if using V100\n",
    "    # logging_steps=10,\n",
    "    optim=\"adamw_torch\",\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    # save_strategy=\"steps\",\n",
    "    # eval_steps=4,\n",
    "    # save_steps=10,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    # save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3378fbc1-a29f-41d0-acb5-5b313baa94ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = transformers.DataCollatorForSeq2Seq(\n",
    "    tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b6d5539-3ac8-4862-81a4-4bfb1e9e3421",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mtransformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_arguments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      9\u001b[0m old_state_dict \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mstate_dict\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:506\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;66;03m# Bnb Quantized models doesn't support `.to` operation.\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplace_model_on_device\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mBITS_AND_BYTES\n\u001b[1;32m    505\u001b[0m ):\n\u001b[0;32m--> 506\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_move_model_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_model_parallel:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:730\u001b[0m, in \u001b[0;36mTrainer._move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_move_model_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, device):\n\u001b[0;32m--> 730\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device)\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mparallel_mode \u001b[38;5;241m==\u001b[39m ParallelMode\u001b[38;5;241m.\u001b[39mTPU \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtie_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=training_arguments,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "model.config.use_cache = False\n",
    "old_state_dict = model.state_dict\n",
    "model.state_dict = (\n",
    "    lambda self, *_, **__: get_peft_model_state_dict(\n",
    "        self, old_state_dict()\n",
    "    )\n",
    ").__get__(model, type(model))\n",
    "\n",
    "model = torch.compile(model)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cae199f-fe8d-4cd0-946b-0650d07c22cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1aba56b0-71f6-4176-9505-56ad041e1c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.generation.utils import GreedySearchDecoderOnlyOutput\n",
    "from peft import PeftModel\n",
    "from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc488ae4-e1d1-4021-851c-8c2571ffb97f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "577f85ae092e4a3a8fdaea9206f1bb7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BASE_MODEL = \"model/llama2-7B-hf\"\n",
    "LORA_WEIGHTS = \"model/llama2_epoch3_3in_1out_4\"\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={'': 0},\n",
    "    local_files_only=True,\n",
    ")\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "model = PeftModel.from_pretrained(model, LORA_WEIGHTS, torch_dtype=torch.float16, device_map={'': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "592faa57-0497-43ec-85da-fae67233a8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = f\"\"\"\n",
    "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{\"Given a user's previous trajectories, learn the patterns and dependencies and predict their next trajectory.\"}\n",
    "\n",
    "### Input:\n",
    "{\"[INPUT]\"}\n",
    "\n",
    "### Response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a08d7b2-b5c5-4d12-ba5e-f6034bc646ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Given a user's previous trajectories, learn the patterns and dependencies and predict their next trajectory.\n",
      "\n",
      "### Input:\n",
      "User ID: 6\n",
      "Trajectory 1: [40.7565, -73.988, Monday 0:00, 1] > [40.6901, -73.9818, Monday 2:00, 3] > [40.6904, -73.9853, Monday 3:00, 1] > [40.8155, -73.9584, Monday 4:00, 1] > [40.6901, -73.9818, Monday 20:00, 3] > [40.6881, -73.9805, Monday 20:00, 1] > [40.7795, -73.9558, Monday 21:00, 1] > [40.8278, -73.9258, Tuesday 4:00, 1] > [40.8332, -73.9419, Wednesday 0:00, 2] > [40.6896, -73.9811, Wednesday 20:00, 0] > [40.7086, -73.991, Wednesday 21:00, 1] > [40.7541, -73.9845, Wednesday 23:00, 4] > [40.7586, -73.9817, Thursday 1:00, 1] > [40.7116, -74.0103, Thursday 3:00, 1] > [40.8341, -73.9453, Thursday 4:00, 0]\n",
      "Trajectory 2: [40.8213, -73.954, Monday 12:00, 1] > [40.6901, -73.9818, Monday 13:00, 3] > [40.6898, -73.9811, Monday 14:00, 0] > [40.689, -73.9813, Monday 16:00, 1] > [40.6622, -73.8648, Monday 17:00, 1] > [40.6963, -73.9907, Tuesday 21:00, 5] > [40.6888, -73.9806, Tuesday 22:00, 0] > [40.7086, -73.991, Wednesday 16:00, 1] > [40.7711, -73.9177, Wednesday 17:00, 1] > [40.7661, -73.8835, Wednesday 18:00, 1] > [40.8033, -73.9328, Wednesday 21:00, 1] > [40.6904, -73.9853, Wednesday 22:00, 1] > [40.6901, -73.9818, Thursday 16:00, 3] > [40.6899, -73.9815, Thursday 19:00, 1] > [40.6963, -73.9907, Thursday 21:00, 5] > [40.6888, -73.9806, Thursday 22:00, 0] > [40.7662, -73.9776, Thursday 23:00, 1] > [40.8332, -73.9419, Sunday 0:00, 2] > [40.6901, -73.9818, Sunday 9:00, 3] > [40.7137, -73.8303, Sunday 13:00, 1] > [40.6901, -73.9841, Sunday 16:00, 0] > [40.808, -73.9639, Sunday 17:00, 1]\n",
      "Trajectory 3: [40.8332, -73.9419, Thursday 3:00, 2] > [40.6901, -73.9818, Thursday 16:00, 3] > [40.7701, -73.918, Thursday 17:00, 1] > [40.7738, -73.8712, Thursday 19:00, 1] > [40.8332, -73.9419, Friday 0:00, 2] > [40.6997, -73.8078, Friday 19:00, 1] > [40.6901, -73.9818, Friday 22:00, 3] > [40.6904, -73.9853, Friday 22:00, 1] > [40.8332, -73.9419, Saturday 17:00, 2] > [40.7086, -73.991, Sunday 18:00, 1] > [40.8332, -73.9419, Sunday 23:00, 2]\n",
      " Trajectory 4: [40.8332, -73.9419, Monday 13:00, 2] > [40.6307, -73.9771, Monday 21:00, 1] > [40.6348, -74.0235, Tuesday 1:00, 1] > [40.6901, -73.9818, Tuesday 3:00, 3] > [40.7565, -73.988, Tuesday 4:00, 1] > [40.7601, -73.9992, Tuesday 5:00, 1] > [40.7318, -73.9857, Tuesday 6:00, 0] > [40.7349, -73.9912, Tuesday 7:00, 1] > [40.72, -73.9788, Tuesday 8:00, 1] > [40.7378, -74.0002, Wednesday 23:00, 1] > [40.7269, -74.0059, Thursday 0:00, 1] > [40.6888, -73.9806, Thursday 4:00, 0] > [40.8278, -73.9258, Thursday 5:00, 1] > [40.8332, -73.9419, Thursday 6:00, 2] > [40.7565, -73.988, Friday 0:00, 1] > [40.6901, -73.9818, Friday 2:00, 3] > [40.6899, -73.9815, Friday 3:00, 1] > [40.8155, -73.9584, Friday 5:00, 1]\n",
      "\n",
      "### Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_prompt(instruction: str) -> str:\n",
    "    return PROMPT_TEMPLATE.replace(\"[INPUT]\", instruction)\n",
    "\n",
    "print(create_prompt('User ID: 6\\nTrajectory 1: [40.7565, -73.988, Monday 0:00, 1] > [40.6901, -73.9818, Monday 2:00, 3] > [40.6904, -73.9853, Monday 3:00, 1] > [40.8155, -73.9584, Monday 4:00, 1] > [40.6901, -73.9818, Monday 20:00, 3] > [40.6881, -73.9805, Monday 20:00, 1] > [40.7795, -73.9558, Monday 21:00, 1] > [40.8278, -73.9258, Tuesday 4:00, 1] > [40.8332, -73.9419, Wednesday 0:00, 2] > [40.6896, -73.9811, Wednesday 20:00, 0] > [40.7086, -73.991, Wednesday 21:00, 1] > [40.7541, -73.9845, Wednesday 23:00, 4] > [40.7586, -73.9817, Thursday 1:00, 1] > [40.7116, -74.0103, Thursday 3:00, 1] > [40.8341, -73.9453, Thursday 4:00, 0]\\nTrajectory 2: [40.8213, -73.954, Monday 12:00, 1] > [40.6901, -73.9818, Monday 13:00, 3] > [40.6898, -73.9811, Monday 14:00, 0] > [40.689, -73.9813, Monday 16:00, 1] > [40.6622, -73.8648, Monday 17:00, 1] > [40.6963, -73.9907, Tuesday 21:00, 5] > [40.6888, -73.9806, Tuesday 22:00, 0] > [40.7086, -73.991, Wednesday 16:00, 1] > [40.7711, -73.9177, Wednesday 17:00, 1] > [40.7661, -73.8835, Wednesday 18:00, 1] > [40.8033, -73.9328, Wednesday 21:00, 1] > [40.6904, -73.9853, Wednesday 22:00, 1] > [40.6901, -73.9818, Thursday 16:00, 3] > [40.6899, -73.9815, Thursday 19:00, 1] > [40.6963, -73.9907, Thursday 21:00, 5] > [40.6888, -73.9806, Thursday 22:00, 0] > [40.7662, -73.9776, Thursday 23:00, 1] > [40.8332, -73.9419, Sunday 0:00, 2] > [40.6901, -73.9818, Sunday 9:00, 3] > [40.7137, -73.8303, Sunday 13:00, 1] > [40.6901, -73.9841, Sunday 16:00, 0] > [40.808, -73.9639, Sunday 17:00, 1]\\nTrajectory 3: [40.8332, -73.9419, Thursday 3:00, 2] > [40.6901, -73.9818, Thursday 16:00, 3] > [40.7701, -73.918, Thursday 17:00, 1] > [40.7738, -73.8712, Thursday 19:00, 1] > [40.8332, -73.9419, Friday 0:00, 2] > [40.6997, -73.8078, Friday 19:00, 1] > [40.6901, -73.9818, Friday 22:00, 3] > [40.6904, -73.9853, Friday 22:00, 1] > [40.8332, -73.9419, Saturday 17:00, 2] > [40.7086, -73.991, Sunday 18:00, 1] > [40.8332, -73.9419, Sunday 23:00, 2]\\n Trajectory 4: [40.8332, -73.9419, Monday 13:00, 2] > [40.6307, -73.9771, Monday 21:00, 1] > [40.6348, -74.0235, Tuesday 1:00, 1] > [40.6901, -73.9818, Tuesday 3:00, 3] > [40.7565, -73.988, Tuesday 4:00, 1] > [40.7601, -73.9992, Tuesday 5:00, 1] > [40.7318, -73.9857, Tuesday 6:00, 0] > [40.7349, -73.9912, Tuesday 7:00, 1] > [40.72, -73.9788, Tuesday 8:00, 1] > [40.7378, -74.0002, Wednesday 23:00, 1] > [40.7269, -74.0059, Thursday 0:00, 1] > [40.6888, -73.9806, Thursday 4:00, 0] > [40.8278, -73.9258, Thursday 5:00, 1] > [40.8332, -73.9419, Thursday 6:00, 2] > [40.7565, -73.988, Friday 0:00, 1] > [40.6901, -73.9818, Friday 2:00, 3] > [40.6899, -73.9815, Friday 3:00, 1] > [40.8155, -73.9584, Friday 5:00, 1]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "214da7de-b890-4a65-8d4e-1d29c39aec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt: str, model: PeftModel) -> GreedySearchDecoderOnlyOutput:\n",
    "    encoding = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = encoding[\"input_ids\"].to(DEVICE)\n",
    "\n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        top_p=0.75,\n",
    "        repetition_penalty=1.1,\n",
    "    )\n",
    "    with torch.inference_mode():\n",
    "        return model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=1024,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48bcd05a-5ce6-4eea-b56a-48d85e6310ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_response(response: GreedySearchDecoderOnlyOutput) -> str:\n",
    "    decoded_output = tokenizer.decode(response.sequences[0])\n",
    "    response = decoded_output.split(\"### Response:\")[1].strip()\n",
    "    return \"\\n\".join(textwrap.wrap(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "015ab244-dd2b-46c5-bdf4-d602ecba713a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_alpaca(prompt: str, model: PeftModel = model) -> str:\n",
    "    prompt = create_prompt(prompt)\n",
    "    response = generate_response(prompt, model)\n",
    "    print(format_response(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac4c5996-3a3d-4b5a-ba10-b25beeb65c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted trajectory for User ID 6 is:  [40.7565, -73.988, Monday\n",
      "0:00, 1] > [40.6901, -73.9818, Monday 2:00, 3] > [40.6904, -73.9853,\n",
      "Monday 3:00, 1] > [40.8155, -73.9584, Monday 4:00, 1] > [40.6901,\n",
      "-73.9818, Monday 20:00, 3] > [40.6881, -73.9805, Monday 20:00, 1] >\n",
      "[40.7795, -73.9558, Monday 21:00, 1] > [40.8278, -73.9258, Tuesday\n",
      "4:00, 1] > [40.8332, -73.9419, Wednesday 0:00, 2] > [40.6896,\n",
      "-73.9811, Wednesday 20:00, 0] > [40.7086, -74.0103, Wednesday 21:00,\n",
      "1] > [40.7541, -73.9845, Wednesday 23:00, 4] > [40.7586, -73.9817,\n",
      "Thursday 1:00, 1] > [40.7116, -74.0103, Thursday 3:00, 1] > [40.8341,\n",
      "-73.9453, Thursday 4:00, 0] > [40.7116, -74.0103, Friday 3:00, 1] >\n",
      "[40.6997, -73.8078, Friday 19:00, 1] > [40.6901, -73.9818, Friday\n",
      "22:00, 3] > [40.6904, -73.9853, Friday 22:00, 1] > [40.8332, -73.9419,\n",
      "Saturday 17:00, 2] > [40.7086, -73.991, Sunday 18:00, 1] > [40.8332,\n",
      "-73.9419, Sunday 23:00, 2]</s>\n"
     ]
    }
   ],
   "source": [
    "ask_alpaca('User ID: 6\\nTrajectory 1: [40.7565, -73.988, Monday 0:00, 1] > [40.6901, -73.9818, Monday 2:00, 3] > [40.6904, -73.9853, Monday 3:00, 1] > [40.8155, -73.9584, Monday 4:00, 1] > [40.6901, -73.9818, Monday 20:00, 3] > [40.6881, -73.9805, Monday 20:00, 1] > [40.7795, -73.9558, Monday 21:00, 1] > [40.8278, -73.9258, Tuesday 4:00, 1] > [40.8332, -73.9419, Wednesday 0:00, 2] > [40.6896, -73.9811, Wednesday 20:00, 0] > [40.7086, -73.991, Wednesday 21:00, 1] > [40.7541, -73.9845, Wednesday 23:00, 4] > [40.7586, -73.9817, Thursday 1:00, 1] > [40.7116, -74.0103, Thursday 3:00, 1] > [40.8341, -73.9453, Thursday 4:00, 0]\\nTrajectory 2: [40.8332, -73.9419, Thursday 3:00, 2] > [40.6901, -73.9818, Thursday 16:00, 3] > [40.7701, -73.918, Thursday 17:00, 1] > [40.7738, -73.8712, Thursday 19:00, 1] > [40.8332, -73.9419, Friday 0:00, 2] > [40.6997, -73.8078, Friday 19:00, 1] > [40.6901, -73.9818, Friday 22:00, 3] > [40.6904, -73.9853, Friday 22:00, 1] > [40.8332, -73.9419, Saturday 17:00, 2] > [40.7086, -73.991, Sunday 18:00, 1] > [40.8332, -73.9419, Sunday 23:00, 2]\\n Trajectory 3: [40.8332, -73.9419, Monday 13:00, 2] > [40.6307, -73.9771, Monday 21:00, 1] > [40.6348, -74.0235, Tuesday 1:00, 1] > [40.6901, -73.9818, Tuesday 3:00, 3] > [40.7565, -73.988, Tuesday 4:00, 1] > [40.7601, -73.9992, Tuesday 5:00, 1] > [40.7318, -73.9857, Tuesday 6:00, 0] > [40.7349, -73.9912, Tuesday 7:00, 1] > [40.72, -73.9788, Tuesday 8:00, 1] > [40.7378, -74.0002, Wednesday 23:00, 1] > [40.7269, -74.0059, Thursday 0:00, 1] > [40.6888, -73.9806, Thursday 4:00, 0] > [40.8278, -73.9258, Thursday 5:00, 1] > [40.8332, -73.9419, Thursday 6:00, 2] > [40.7565, -73.988, Friday 0:00, 1] > [40.6901, -73.9818, Friday 2:00, 3] > [40.6899, -73.9815, Friday 3:00, 1] > [40.8155, -73.9584, Friday 5:00, 1]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b058705e-77ec-438e-9c3b-213859f83ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
